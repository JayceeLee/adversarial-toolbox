import sys
sys.path.append('../')

from keras.utils import to_categorical
from keras.models import Model
from keras.layers.core import Dense, Dropout, Flatten
from keras.layers import GlobalAveragePooling2D

from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import keras.backend as K
import numpy as np
from scipy.ndimage import filters
from skimage import color
import skimage.filters
import plot_gradients as pg
#from models.resnet50 import ResNet50
from keras.applications.resnet50 import preprocess_input, decode_predictions
from keras.applications.resnet50 import ResNet50

weights_top = '../models/weights/base_models/resnet50_full.h5'
weights_notop = '../models/weights/base_models/resnet50_notop.h5'


def load_model(top='vanilla', weight_path=None):

    if top == 'vanilla':
        # model = ResNet50(top=top, input_shape=(224, 224, 3))
        model = ResNet50(weights='imagenet')
        # model.load_weights(weights_top)
    elif top == 'detector':
        # model = ResNet50(top=top, input_shape=(224, 224, 3), classes=2)
        if weight_path is None:
            model = ResNet50(weights='imagenet', include_top=False)
            # model.load_weights(weights_notop)
        else:
            model = ResNet50(weights=None, include_top=False)
            model.load_weights(weight_path)

    else:
        raise ValueError('Bad input for top of gcnn, choose vanilla or detector')
    return model


def collect_gradients(data, dim):

    # Using three channels seems to be too sparse, one channel works
    # But we're going to give it more of and effort with large images
    grad_data = np.empty((data.shape))
    for i in range(len(data)):
        im = data[i].astype(np.int32)
        """
        im = color.rgb2gray(im)
	imx = np.zeros(im.shape)
        filters.sobel(im, 1, imx)
        imy = np.zeros(im.shape)
        filters.sobel(im, 0, imy)
        magnitude = np.sqrt(imx**2+imy**2)
	plt.imshow(magnitude)
	plt.show()
	print magnitude
	sys.exit(0)
        """
        #im = skimage.filters.gaussian(im, sigma=2, multichannel=True)

        gradients = np.zeros(im.shape)
        channels = im.shape[2]
        for j in range(channels):
	    imx = np.zeros((224, 224))
            filters.sobel(im[:, :, j], 1, imx)
            imy = np.zeros((224, 224))
            filters.sobel(im[:, :, j], 0, imy)

            mag = np.sqrt(imx**2+imy**2)
            gradients[:, :, j] = mag

        grad_data[i] = gradients

    print "\n==> gradient data shape\n", grad_data.shape

    return grad_data


def display_predict(model, x, y):

    for im, label in zip(x, y):
        if label[0] == 1:
            z = np.expand_dims(im, axis=0)
            preds = model.predict(z)
            print('Predicted:', decode_predictions(preds, top=3)[0])
            plt.imshow(z[0])
            plt.show()


def train_gcnn(model, train, test):

    d = train[0].shape[1]
    X_train, Y_train = train
    X_test, Y_val = test
    X_train = collect_gradients(X_train, d)
    X_val = collect_gradients(X_test, d)

    print "training data shape: ", X_train.shape
    # cnn model training

    acc = 0
    conv_acc = 0
    iterations = 0
    while acc < 0.7:
        print "Training iteration ", iterations
        if iterations > 200:
            print "Number of training iterations too great, exiting"
            sys.exit(0)

        model = load_model(top='detector')
        base = model.output

        print Y_train.shape
        x = GlobalAveragePooling2D()(base)
        #x = Dense(1024, kernel_initializer='glorot_normal', activation='relu')(x)
        x = Dense(256, kernel_initializer='glorot_normal', activation='relu')(x)
        x = Dropout(.5)(x)

        preds = Dense(2, kernel_initializer='glorot_normal', activation='softmax')(x)

        for layer in model.layers:
            layer.trainable = False

        model = Model(model.input, preds)

        for i, layer in enumerate(model.layers):
            print(i, layer.name)

        model.compile(optimizer='adam',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

        print "==> Tuning added FC layers"
        hist = model.fit(X_train,
                         Y_train,
                         epochs=20,
                         batch_size=8,
                         shuffle=True,
                         validation_data=(X_val, Y_val))

        acc = hist.history['acc'][-1]
    model.save_weights('/tmp/fc_params.h5')
    while conv_acc < 0.7:

        model.load_weights('/tmp/fc_params.h5')
        for layer in model.layers[170]:
            layer.trainable = False
        for layer in model.layers[170]:
            layer.trainable = True

        model.compile(optimizer='sgd',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

        hist = model.fit(X_train,
                         Y_train,
                         epochs=20,
                         batch_size=16,
                         shuffle=True,
                         validation_data=(X_val, Y_val))

        conv_acc = hist.history['acc'][-1]
        print "accuracy: {}".format(conv_acc)
    return model, (X_test_grad, Y_val)


def test_svm(clf, data, plot=False):

    correct, fp, fn = 0, 0, 0
    real, adv = [], []
    c_grads, n_grads = [], []
    x_test, y_test = data

    for i, sample in enumerate(x_test):
        if y_test[i] == 0.:
            adv.append(sample)
        elif y_test[i] == 1.:
            real.append(sample)
        pred = clf.predict(sample)[0]
        if pred == y_test[i]:
            # print "correct -- pred: {}\t label: {}".format(pred, y_test[i])
            correct += 1.
            c_grads.append(sample)
        else:
            # print "incorrect -- pred: {}\t label: {}".format(pred, y_test[i])
            if pred == 0:
                fn += 1.
            if pred == 1:
                fp += 1.
            n_grads.append(sample)

    # print "\nACC: {}, {}".format(correct / len(x_test), correct)
    # print "False Negative: {}, {}".format(fn/len(x_test), fn)
    # print "False Positive: {}, {}".format(fp/len(x_test), fp)
    if plot:
        pg.plot_pdf([c_grads, n_grads], ["positive", "negative"])
    return correct/len(x_test)
